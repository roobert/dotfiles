# RPI / Docker / Grafana / InfluxDB / MQQT / Victron / Tasmoto - Power Monitoring System (work in progress/notes)

# disable swap
sudo dphys-swapfile swapoff
sudo dphys-swapfile uninstall
sudo update-rc.d dphys-swapfile remove
sudo apt purge dphys-swapfile

# install log2ram
echo "deb [signed-by=/usr/share/keyrings/azlux-archive-keyring.gpg] http://packages.azlux.fr/debian/ bullseye main" | sudo tee /etc/apt/sources.list.d/azlux.list
sudo wget -O /usr/share/keyrings/azlux-archive-keyring.gpg  https://azlux.fr/repo.gpg
sudo apt update
sudo apt install log2ram

## edit /etc/log2ram.conf
MAIL=false
# du -sh /var/log ...
SIZE=512M

# TODO:
* sync data to remote?
* compare version of grafana and shit to what is in latest x86 version..

# Docker
*  install docker-ce not docker from apt..
```
curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
sudo apt -y install docker-ce docker-ce-cli containerd.io
```

* install docker-compose from github
```
wget https://github.com/docker/compose/releases/download/v2.6.1/docker-compose-linux-aarch64
mv docker-compose-linux-aarch64 bin/docker-compose
sudo systemctl enable docker
```

* permit our user to use docker
```
sudo service start docker
sudo usermod -aG docker rw
```

# download components

# make the following changes to venus-docker-grafana to allow grafana to scrape metrics
from plug influxdb instance
```
# for each service, add:
    restart: always
    networks:
      - network0

# add a network block:
networks:
  network0:
    name: network0
    external: true

# change grafana docker tag from latest to: armhf-2.0.0
# change upnp and server docker tags from latest to: armhf-latest

# add `expose:` lines to docker-compose and copy ports from `ports:` key to open up
access to services via network

```

```
sudo docker network create network0
```

# data is stored in /var/lib/docker/volumes/

# Once services are running, configure a new backend with:
name: influxdb-plugs
address: influxdb-plugs:18086
database: plugs

# configure the localbytes/tasmota plugs
* write traffic to rpi address
* adjust how often the plugs write data - 60s

# Add autossh and forward ports from pi to colo VM since 4G means router is not on
public network
generate ssh key on rpi, copy authorized key to dust

sudo systemctl enable autossh
sudo service autossh start

add systemd autossh.service init script:

```
[Unit]
Description=AutoSSH service
After=network.target

[Service]
Environment="AUTOSSH_GATETIME=0"
User=rw
Group=rw
ExecStart=/usr/bin/autossh -N -M 0 -o "ServerAliveInterval 5" -o "ServerAliveCountMax 3" -i /home/rw/.ssh/id_rsa -R 6969:127.0.0.1:22 rw@dust.cx
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
```

4. Go to the admin interface @ http://localhost:8088, default username and password are admin and admin.
5. Accessing Grafana on http://localhost:3000 and enter `admin` for user name and `admin` for password.


# login to the victron site hosted on the pi and add the einstine/cervo thing to discovery
page by IP

# login to router and reserve the leases to addresses for the various devices don't
change..

# change the influxdb retention period.. through the Venus UI - for cerbo data, and
through ??? for plug data? 30 years: 10950d
# mount the data volume on external SSD

# change dashboard colour to light


docker ps
docker exec <grafana container id> grafana-cli plugins install pr0ps-trackmap-panel
